{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f208ff-fde3-4703-8a63-c0dba654815f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RL Project\n",
    "This Jupyter notebook is a base for importing the [Arcade Learning Environment](https://ale.farama.org/getting-started/#) (ALE) which is part of the [Gymnasium](https://gymnasium.farama.org) project. ALE can be used to create an environment for various Atari 2600 games for training reinforcement learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T14:24:43.287626Z",
     "start_time": "2025-11-29T14:24:42.075863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in /opt/miniconda3/lib/python3.12/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /opt/miniconda3/lib/python3.12/site-packages (from ale-py) (2.2.6)\n",
      "Requirement already satisfied: gymnasium[other] in /opt/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (0.0.4)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (4.12.0.88)\n",
      "Requirement already satisfied: seaborn>=0.13 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (5.2.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (2.37.2)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from proglog<=1.0.0->moviepy>=1.0.0->gymnasium[other]) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[other]) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/miniconda3/lib/python3.12/site-packages (from seaborn>=0.13->gymnasium[other]) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ale-py\n",
    "!pip install \"gymnasium[other]\"\n",
    "# !pip install --user --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a19c0-ace5-491d-a37e-ecfe9b0b54c1",
   "metadata": {},
   "source": [
    "## Import the native ALE interface as ale_py\n",
    "Taken from <https://ale.farama.org/getting-started/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445b92db-fc9f-4d1a-b967-7700bcf9e366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T11:15:24.549770Z",
     "start_time": "2025-11-30T11:15:24.498344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057ba19-05f7-4256-914d-e1a67f7ddd74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47664e-9df9-4054-b52e-69153c9f4bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "da19c747-5220-49c0-a6a5-6c2543a373b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\" Stores the replay buffer referred to as D in the pseudo-code \"\"\"\n",
    "\n",
    "    def __init__(self, envs, max_size = 1000000, start_size = 50000, frame_height = 84, frame_width = 84, agent_history_length = 4):\n",
    "        self.max_size = max_size\n",
    "        self.count = 0\n",
    "        self.max_count = 0\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "        self.actions = np.empty(self.max_size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.max_size, agent_history_length, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.rewards = np.empty(self.max_size, dtype=np.float32)\n",
    "        self.terminations = np.empty(self.max_size, dtype=np.bool)\n",
    "        \n",
    "        # We initialise the ReplayMemory with some initial samples\n",
    "        for _ in range(start_size):\n",
    "            # Take random actions in all environments\n",
    "            actions = envs.action_space.sample()\n",
    "            step_data = envs.step(actions)\n",
    "            self.add_step_data(actions, step_data)\n",
    "\n",
    "    def add_step_data(self, actions, step_data):\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = step_data\n",
    "\n",
    "        # We're using the vector based Atari environment but only initialising 1 instance so take the first element of each variable\n",
    "        # print(f\"adding count {self.count} action {actions[0]} observations {observations[0].shape} rewards {rewards[0]} terminations {terminations[0]}\")\n",
    "        self.actions[self.count] = actions[0]\n",
    "        self.frames[self.count] = observations[0]\n",
    "        self.rewards[self.count] = rewards[0]\n",
    "        self.terminations[self.count] = terminations[0]\n",
    "        self.count += 1 \n",
    "        self.count = self.count % self.max_size\n",
    "        self.max_count = max(self.max_count, self.count)\n",
    "        \n",
    "    def get_random_minibatch(self):\n",
    "        print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce162f-5f00-46c2-b9bf-65c59a8f99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "59e9d0b7-fde8-4a19-9451-e287a0ceaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hyperparameters and their values from Mnih 2015\n",
    "minibatch_size          = 32          # Number of training cases over which each SGD update is computed.\n",
    "replay_memory_size      = 1000000     # SGD udpates are sampled from this number of most recent frames.\n",
    "agent_history_length    = 4           # The number of most recent frames that are given to the Q‚Äënetwork.\n",
    "target_update_freq      = 10000       # The frequency with which the target network is updated.\n",
    "discount_factor         = 0.99        # Discount factor Œ≥ used in Q‚Äëlearning update.\n",
    "action_repeat           = 4           # Repeat each selected action this many times.\n",
    "update_frequency        = 4           # The number of actions selected by the agent between SGD updates.\n",
    "learning_rate           = 2.5e-4      # The learning rate used by RMSProp.\n",
    "gradient_momentum       = 0.95        # Gradient momentum used by RMSProp.\n",
    "squared_grad_momentum   = 0.95        # Squared‚Äëgradient (denominator) momentum used by RMSProp.\n",
    "min_squared_gradient    = 0.01        # Constant added to squared gradient in the denominator of the RMSProp update.\n",
    "initial_exploration     = 1.0         # Initial Œµ in Œµ‚Äëgreedy exploration.\n",
    "final_exploration       = 0.1         # Final value of Œµ in Œµ‚Äëgreedy exploration.\n",
    "final_exploration_frame = 1000000     # Number of frames over which the initial value of Œµ is linearly annealed to its final value. 1,000,000 in Mnih\n",
    "replay_start_size       = 50000       # A uniform random policy is run for this number of frames before learning starts and is used to populate the replay memory. 50,000 in Mnih.\n",
    "noop_max                = 30          # Max number of \"do nothing\" actions performed by the agent at the start of an episode.\n",
    "\n",
    "frame_height            = 84\n",
    "frame_width             = 84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5715c7-779b-446c-a385-28694d04b7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "874a96bb-f090-4e65-823e-c22003472f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressing fire 6 times\n",
      "epsiode 0\n",
      "epsiode 1\n",
      "epsiode 2\n",
      "epsiode 3\n",
      "epsiode 4\n",
      "epsiode 5\n",
      "epsiode 6\n",
      "epsiode 7\n",
      "epsiode 8\n",
      "epsiode 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ale_py.vector_env import AtariVectorEnv\n",
    "\n",
    "# *** TODO REMOVE THESE LINES ONCE TESTED\n",
    "replay_memory_size      = 100\n",
    "replay_start_size       = 80\n",
    "\n",
    "def initialise_vector_environment():\n",
    "    \"\"\" Initialises the environment \"\"\" \n",
    "        \n",
    "    # Create a vector environment with a single instance of Breakout, the vector environment provides a number of convenient parameters:\n",
    "    envs = AtariVectorEnv(\n",
    "        game=\"pong\",  # The ROM id not name, i.e., camel case compared to `gymnasium.make` name versions\n",
    "        num_envs = 1,                     # The number of parallel environments (we just use 1)\n",
    "        grayscale = True,                 # Reduce memory by converting to grayscale (from Mnih 2015)\n",
    "        reward_clipping = True,           # Restricting the rewards to -1, 0 or +1 (as per Mnih 2015)\n",
    "        stack_num = agent_history_length, # Number of frames to stack (as per Mnih 2015)\n",
    "        img_height = frame_height,        # Height to resize frames to (as per Mnih 2015)\n",
    "        img_width = frame_width,          # Width to resize frames to (as per Mnih 2015)\n",
    "    )\n",
    "\n",
    "    observations, info = envs.reset(seed=42)\n",
    "\n",
    "    # Atari games are deterministic. Press fire a random umber of times to prevent the agent simply learning a single ideal path.\n",
    "    FIRE = 1\n",
    "    action = np.array([FIRE], dtype=np.int32)\n",
    "    n = random.randint(0, noop_max)\n",
    "    print(f\"Pressing fire {n} times\")\n",
    "    for _ in range(n):\n",
    "        step_data = envs.step(action)\n",
    "\n",
    "    return envs\n",
    "\n",
    "envs = initialise_vector_environment()\n",
    "\n",
    "\n",
    "# *** Based on pseudo code for Deep Q-learning with Experience Replay (from the course notes) ***\n",
    "\n",
    "# Initialise replay memory ùê∑ to capacity ùëÅ\n",
    "\n",
    "# Reset all environments\n",
    "observations, info = envs.reset(seed=42)\n",
    "\n",
    "D = ReplayMemory(envs, replay_memory_size, replay_start_size, frame_height, frame_width, agent_history_length)\n",
    "\n",
    "# Initialise action-value network ùëûÃÇ1 with parameters ùúΩ1 ‚àà ‚Ñùùëë arbitrarily \n",
    "\n",
    "# Initialise target action-value network ùëûÃÇ2 with parameters ùúΩ2 = ùúΩ1\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "# Loop for each episode:\n",
    "for episode in range(0, num_episodes):\n",
    "\n",
    "    print(f\"epsiode {episode}\")\n",
    "    # Initialise ùëÜ\n",
    "\n",
    "    # Loop for each step of episode:\n",
    "    while (not terminations[0]) :\n",
    "    \n",
    "        # Choose action ùê¥ in state ùëÜ using policy derived from ùëûÃÇ1(ùëÜ,‚ãÖ,ùúÉ1) \n",
    "        # *** TODO Choose the action accordin to the policy q1 \n",
    "        #action = envs.action_space.sample()\n",
    "        FIRE = 1\n",
    "        action = np.array([FIRE], dtype=np.int32)\n",
    "\n",
    "        # Take action ùê¥, observe reward ùëÖ and next-state ùëÜ‚Ä≤ \n",
    "        step_data = envs.step(actions)\n",
    "\n",
    "        # Store transition (ùëÜ,ùê¥,ùëÖ,ùëÜ‚Ä≤) in ùê∑\n",
    "        D.add_step_data(actions, step_data)\n",
    "        observations, rewards, terminations, truncations, infos = step_data\n",
    "\n",
    "        # For each transition (ùëÜùëó,ùê¥ùëó,ùëÖùëó,ùëÜ‚Ä≤ùëó) in minibatch sampled from ùê∑:\n",
    "            # ùë¶={ùëÖùëó if ùëÜ‚Ä≤ùëó is terminal, ùëÖùëó+ùõæmaxùëé‚Ä≤ùëûÃÇ2(ùëÜ‚Ä≤ùëó,ùëé‚Ä≤,ùúΩ2) otherwise \n",
    "            # ùë¶ÃÇ=ùëûÃÇ1(ùëÜùëó,ùê¥ùëó,ùúΩ1) \n",
    "            # Perform gradient descent step ‚àáùúÉ1ùêøùõø(ùë¶,ùë¶ÃÇ) \n",
    "\n",
    "        if terminations[0] or truncations[0]:\n",
    "            observation, info = envs.reset()\n",
    "\n",
    "    # Every ùê∂ time-steps, update ùúΩ2=ùúΩ1\n",
    "\n",
    "# Close the environment when done\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310412cf-616e-40e2-bf92-4ca4e593a652",
   "metadata": {},
   "source": [
    "## Inline rendering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "55067009-a40a-49cc-96bd-c1e7e5100a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Reset all environments\n",
    "observations, info = envs.reset()\n",
    "\n",
    "print(observations.shape)\n",
    "\n",
    "# Take random actions in all environments\n",
    "actions = envs.action_space.sample()\n",
    "observations, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "for step in range(100):\n",
    "    # Render the environment  \n",
    "    for i in range(0, 4):\n",
    "        img = Image.fromarray(observations[0][i].astype('uint8'))\n",
    "        #img = img.resize((336, 336), resample=Image.NEAREST)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Choose a random action, this is where we would insert a policy\n",
    "    action = envs.action_space.sample()\n",
    "    step_data = envs.step(action)\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = step_data\n",
    "    \n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminations[0] or truncations[0]:\n",
    "        observations, infos = envs.reset()\n",
    "\n",
    "# Close the environment when done\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e6d13-7637-4f09-b812-d27d88a25262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
