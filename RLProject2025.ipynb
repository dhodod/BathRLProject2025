{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f208ff-fde3-4703-8a63-c0dba654815f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RL Project\n",
    "This Jupyter notebook is a base for importing the [Arcade Learning Environment](https://ale.farama.org/getting-started/#) (ALE) which is part of the [Gymnasium](https://gymnasium.farama.org) project. ALE can be used to create an environment for various Atari 2600 games for training reinforcement learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T14:24:43.287626Z",
     "start_time": "2025-11-29T14:24:42.075863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in /opt/miniconda3/lib/python3.12/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /opt/miniconda3/lib/python3.12/site-packages (from ale-py) (2.2.6)\n",
      "Requirement already satisfied: gymnasium[other] in /opt/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (0.0.4)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=3.0 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (4.12.0.88)\n",
      "Requirement already satisfied: seaborn>=0.13 in /opt/miniconda3/lib/python3.12/site-packages (from gymnasium[other]) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.0->gymnasium[other]) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (5.2.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (2.37.2)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from moviepy>=1.0.0->gymnasium[other]) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from proglog<=1.0.0->moviepy>=1.0.0->gymnasium[other]) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[other]) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/miniconda3/lib/python3.12/site-packages (from seaborn>=0.13->gymnasium[other]) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ale-py\n",
    "!pip install \"gymnasium[other]\"\n",
    "# !pip install --user --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a19c0-ace5-491d-a37e-ecfe9b0b54c1",
   "metadata": {},
   "source": [
    "## Import the native ALE interface as ale_py\n",
    "Taken from <https://ale.farama.org/getting-started/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445b92db-fc9f-4d1a-b967-7700bcf9e366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T11:15:24.549770Z",
     "start_time": "2025-11-30T11:15:24.498344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310412cf-616e-40e2-bf92-4ca4e593a652",
   "metadata": {},
   "source": [
    "## Inline rendering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a057ba19-05f7-4256-914d-e1a67f7ddd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renders some sample frames within the Jupyter notebook. \n",
    "# It's not very fast!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make('ALE/Breakout-v5', render_mode='rgb_array')\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for step in range(100):\n",
    "    # Render the environment  \n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # Choose a random action, this is where we would insert a policy\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc52738-1a8f-4f79-9a6a-928c7a32e0f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:50:08.851264Z",
     "start_time": "2025-11-30T10:49:30.970838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Renders some sample frames in a separate window. \n",
    "# Smoother than in inline rendering but closing the window resets the Jupyter notebook kernal\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # Choose a random action, this is where we would insert a policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ce162f-5f00-46c2-b9bf-65c59a8f99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code for Deep Q-learning with Experience Replay (from the course notes)\n",
    "\n",
    "# Initialise replay memory ğ· to capacity ğ‘\n",
    "# Initialise action-value network ğ‘Ì‚1 with parameters ğœ½1 âˆˆ â„ğ‘‘ arbitrarily \n",
    "# Initialise target action-value network ğ‘Ì‚2 with parameters ğœ½2 = ğœ½1\n",
    "\n",
    "# Loop for each episode:  \n",
    "    # Initialise ğ‘† \n",
    "\n",
    "    # Loop for each step of episode:  \n",
    "        # Choose action ğ´ in state ğ‘† using policy derived from ğ‘Ì‚1(ğ‘†,â‹…,ğœƒ1) \n",
    "        # Take action ğ´, observe reward ğ‘… and next-state ğ‘†â€² \n",
    "        # Store transition (ğ‘†,ğ´,ğ‘…,ğ‘†â€²) in ğ· \n",
    "        # For each transition (ğ‘†ğ‘—,ğ´ğ‘—,ğ‘…ğ‘—,ğ‘†â€²ğ‘—) in minibatch sampled from ğ·:\n",
    "            # ğ‘¦={ğ‘…ğ‘— if ğ‘†â€²ğ‘— is terminal, ğ‘…ğ‘—+ğ›¾maxğ‘â€²ğ‘Ì‚2(ğ‘†â€²ğ‘—,ğ‘â€²,ğœ½2) otherwise \n",
    "            # ğ‘¦Ì‚=ğ‘Ì‚1(ğ‘†ğ‘—,ğ´ğ‘—,ğœ½1) \n",
    "            # Perform gradient descent step âˆ‡ğœƒ1ğ¿ğ›¿(ğ‘¦,ğ‘¦Ì‚) \n",
    "\n",
    "    # Every ğ¶ time-steps, update ğœ½2=ğœ½1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e9d0b7-fde8-4a19-9451-e287a0ceaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hyperparameters and their values (from the Google DeepMind paper)\n",
    "\n",
    "minibatch_size          = 32          # Number of training cases over which each SGD update is computed.\n",
    "replay_memory_size      = 1000000     # SGD udpates are sampled from this number of most recent frames.\n",
    "agent_history_length    = 4           # The number of most recent frames that are given to the Qâ€‘network.\n",
    "target_update_freq      = 10000       # The frequency with which the target network is updated.\n",
    "discount_factor         = 0.99        # Discount factor Î³ used in Qâ€‘learning update.\n",
    "action_repeat           = 4           # Repeat each selected action this many times.\n",
    "update_frequency        = 4           # The number of actions selected by the agent between SGD updates.\n",
    "learning_rate           = 2.5e-4      # The learning rate used by RMSProp.\n",
    "gradient_momentum       = 0.95        # Gradient momentum used by RMSProp.\n",
    "squared_grad_momentum   = 0.95        # Squaredâ€‘gradient (denominator) momentum used by RMSProp.\n",
    "min_squared_gradient    = 0.01        # Constant added to squared gradient in the denominator of the RMSProp update.\n",
    "initial_exploration     = 1.0         # Initial Îµ in Îµâ€‘greedy exploration.\n",
    "final_exploration       = 0.1         # Final value of Îµ in Îµâ€‘greedy exploration.\n",
    "final_exploration_frame = 1000000     # Number of frames over which the initial value of Îµ is linearly annealed to its final value.\n",
    "replay_start_size       = 50000       # A uniform random policy is run for this number of frames before learning starts and is used to populate the replay memory.\n",
    "noop_max                = 30          # Max number of \"do nothing\" actions performed by the agent at the start of an episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5715c7-779b-446c-a385-28694d04b7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94350a-5655-4500-8efc-6b82ba469b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
